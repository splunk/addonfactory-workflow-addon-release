# jscpd:ignore-start
name: build-test-release
on:
  workflow_call:
    inputs:
      marker:
        required: false
        description: 'Parallel run marker'
        type: string
        default: >-
          [""]
    secrets:
      GH_TOKEN_ADMIN:
        description: Github admin token
        required: true
      GH_TOKEN:
        description: Github token
        required: true
      SEMGREP_PUBLISH_TOKEN:
        description: Semgrep token
        required: true
      AWS_ACCESS_KEY_ID:
        description: AWS access key id
        required: true
      AWS_DEFAULT_REGION:
        description: AWS default region
        required: true
      AWS_SECRET_ACCESS_KEY:
        description: AWS secret access key
        required: true
      VT_API_KEY:
        description: Virustotal api key
        required: true
      CODECOV_TOKEN:
        description: Codecov token
        required: true
      OTHER_TA_REQUIRED_CONFIGS:
        description: other required configs
        required: true
      FOSSA_API_KEY:
        description: API token for FOSSA app
        required: true
      SKYNET_TOKEN:
        description: API token for Skynet
        required: false

jobs:
  meta:
    runs-on: ubuntu-latest
    outputs:
      sc4s: ghcr.io/${{ github.repository }}/container:${{ fromJSON(steps.docker_action_meta.outputs.json).labels['org.opencontainers.image.version'] }}
      container_tags: ${{ steps.docker_action_meta.outputs.tags }}
      container_labels: ${{ steps.docker_action_meta.outputs.labels }}
      container_buildtime: ${{ fromJSON(steps.docker_action_meta.outputs.json).labels['org.opencontainers.image.created'] }}
      container_version: ${{ fromJSON(steps.docker_action_meta.outputs.json).labels['org.opencontainers.image.version'] }}
      container_revision: ${{ fromJSON(steps.docker_action_meta.outputs.json).labels['org.opencontainers.image.revision'] }}
      container_base: ${{ fromJSON(steps.docker_action_meta.outputs.json).tags[0] }}
      matrix_supportedSplunk: ${{ steps.matrix.outputs.supportedSplunk }}
      matrix_latestSplunk: ${{ steps.matrix.outputs.latestSplunk }}
      matrix_supportedSC4S: ${{ steps.matrix.outputs.supportedSC4S }}
      matrix_supportedModinputFunctionalVendors: ${{ steps.matrix.outputs.supportedModinputFunctionalVendors }}
      matrix_supportedUIVendors: ${{ steps.matrix.outputs.supportedUIVendors }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3
        with:
          submodules: false
          persist-credentials: false
      - uses: actions/setup-node@v3
        with:
          node-version: '14'
      - name: Semantic Release
        id: version
        uses: cycjimmy/semantic-release-action@v2.7.0
        with:
          semantic_version: 17
          extra_plugins: |
            @semantic-release/exec
            @semantic-release/git
            semantic-release-helm
            @google/semantic-release-replace-plugin
          dry_run: true
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
      - name: Docker meta
        id: docker_action_meta
        uses: docker/metadata-action@v4.0.1
        with:
          images: ghcr.io/${{ github.repository }}/container
          tags: |
            type=sha,format=long
            type=sha
            type=semver,pattern={{version}},value=${{ steps.version.outputs.new_release_version }}
            type=semver,pattern={{major}},value=${{ steps.version.outputs.new_release_version }}
            type=semver,pattern={{major}}.{{minor}},value=${{ steps.version.outputs.new_release_version }}
            type=ref,event=branch
            type=ref,event=pr
      - name: matrix
        id: matrix
        uses: splunk/addonfactory-test-matrix-action@v1.8

  skip-tests:
    runs-on: ubuntu-latest
    outputs:
      skip_tests: ${{ steps.skip-tests.outputs.skip_tests }}
    steps:
      - name: skip stage based on description
        id: skip-tests
        run: |
          set +e
          TESTSET="knowledge ui modinput_functional scripted_inputs escu requirement_test"
          SKIP_TESTS="Yes"
          if [[ '${{ github.event.label.name }}' == 'preserve_infra' ]]; then
            echo "${{ github.event.pull_request.body }}" >> body.txt
            tests=$(grep -i "^preserve:" body.txt | { grep -v grep || true; })
            for i in $TESTSET; do
                if [[ $tests =~ "$i" ]]; then
                    eval SKIP_TESTS="No"
                fi
            done
          fi
          echo "$SKIP_TESTS"
          echo "::set-output name=skip_tests::$SKIP_TESTS"    

  fossa-scan:
    continue-on-error: true
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: run fossa anlyze and create report
        run: |
          curl -H 'Cache-Control: no-cache' https://raw.githubusercontent.com/fossas/fossa-cli/master/install-latest.sh | bash
          fossa analyze --debug
          fossa report attribution --format text --timeout 600 > /tmp/THIRDPARTY
        env:
          FOSSA_API_KEY: ${{ secrets.FOSSA_API_KEY }}
      - name: upload THIRDPARTY file
        uses: actions/upload-artifact@v3
        with:
          name: THIRDPARTY
          path: /tmp/THIRDPARTY
      - name: run fossa test
        run: |
          fossa test --debug
        env:
          FOSSA_API_KEY: ${{ secrets.FOSSA_API_KEY }}

  compliance-copyrights:
    name: compliance-copyrights
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: REUSE Compliance Check
        uses: fsfe/reuse-action@v1.1

  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: "3.7"
      - uses: pre-commit/action@v3.0.0

  review_secrets:
    name: security-detect-secrets
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        if: github.event_name != 'pull_request'
        uses: actions/checkout@v3
        with:
          submodules: false
          fetch-depth: "0"
      - name: Checkout for PR
        if: github.event_name == 'pull_request'
        uses: actions/checkout@v3
        with:
          submodules: false
          fetch-depth: "0"
          ref: ${{ github.head_ref }}
      - name: Trufflehog Actions Scan
        uses: edplato/trufflehog-actions-scan@v0.9l-beta
        with:
          scanArguments: "--max_dept 50 -x .github/workflows/exclude-patterns.txt --allow .github/workflows/trufflehog-false-positive.json"

  semgrep:
    runs-on: ubuntu-latest
    name: security-sast-semgrep
    if: github.actor != 'dependabot[bot]'
    steps:
      - uses: actions/checkout@v3
      - name: Semgrep
        id: semgrep
        uses: returntocorp/semgrep-action@v1
        with:
          publishToken: ${{ secrets.SEMGREP_PUBLISH_TOKEN }}

  build:
    name: build
    runs-on: ubuntu-latest
    needs:
      - fossa-scan
    outputs:
      buildname: ${{ steps.buildupload.outputs.name }}
    steps:
      - uses: actions/checkout@v3
        with:
          # Very Important semantic-release won't trigger a tagged
          # build if this is not set false
          persist-credentials: false
      - name: Setup python
        uses: actions/setup-python@v4
        with:
          python-version: 3.7
      - uses: actions/setup-node@v3
        with:
          node-version: 14
      - name: create requirements file for pip
        run: |
          if [ -f "poetry.lock" ]
          then
            echo " poetry.lock found "
            echo "Value of skip_tests variable is ${{ needs.skip-tests.outputs.skip_tests }}" 
            sudo pip3 install poetry==1.2.2 poetry-plugin-export==1.2.0
            poetry export --without-hashes -o requirements.txt
            if [ "$(grep -cve '^\s*$' requirements.txt)" -ne 0 ]
            then
                echo "Prod dependencies were found, creating package/lib folder"
                mkdir -p package/lib || true
                mv requirements.txt package/lib
            else
                echo "No prod dependencies were found"
                rm requirements.txt
            fi
            poetry export --without-hashes --dev -o requirements_dev.txt
            cat requirements_dev.txt
          fi
      - name: Get pip cache dir
        id: pip-cache
        run: |
          echo "::set-output name=dir::$(pip cache dir)"
      - name: Run Check there are libraries to scan
        id: checklibs
        run: if [ -f requirements_dev.txt ]; then echo "::set-output name=ENABLED::true"; fi
      - name: pip cache
        if: ${{ steps.checklibs.outputs.ENABLED == 'true' }}
        uses: actions/cache@v3
        with:
          path: ${{ steps.pip-cache.outputs.dir }}
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements_dev.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      - name: Install deps
        if: ${{ steps.checklibs.outputs.ENABLED == 'true' }}
        run: pip install -r requirements_dev.txt
      - name: Semantic Release Get Next
        id: semantic
        if: github.event_name != 'pull_request'
        uses: cycjimmy/semantic-release-action@v2.7.0
        with:
          semantic_version: 17
          extra_plugins: |
            @semantic-release/exec
            @semantic-release/git
          dry_run: true
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN_ADMIN }}
      - name: Determine the version to build
        id: BuildVersion
        uses: splunk/addonfactory-get-splunk-package-version-action@v1
        with:
          SemVer: ${{ steps.semantic.outputs.new_release_version }}
          PrNumber: ${{ github.event.number }}
      - name: Download THRIDPARTY
        if: github.event_name != 'pull_request' && github.event_name != 'schedule'
        uses: actions/download-artifact@v3
        with:
          name: THIRDPARTY
      - name: Download THRIDPARTY (Optional for PR and schedule)
        if: github.event_name == 'pull_request' || github.event_name == 'schedule'
        continue-on-error: true
        uses: actions/download-artifact@v3
        with:
          name: THIRDPARTY
      - name: Update Notices
        run: |
          cp -f THIRDPARTY package/THIRDPARTY || echo "THIRDPARTY file not found (allowed for PR and schedule)"
      - name: Build Package
        id: uccgen
        uses: splunk/addonfactory-ucc-generator-action@v1
        with:
          version: ${{ steps.BuildVersion.outputs.VERSION }}
      - name: Slim Package
        id: slim
        uses: splunk/addonfactory-packaging-toolkit-action@v1
        with:
          source: ${{ steps.uccgen.outputs.OUTPUT }}
      - name: artifact-splunk-unpacked
        uses: actions/upload-artifact@v3
        with:
          name: package-raw
          path: ${{ steps.uccgen.outputs.OUTPUT }}**
        if: always()
      - name: artifact-splunk-base
        uses: actions/upload-artifact@v3
        with:
          name: package-splunkbase
          path: ${{ steps.slim.outputs.OUTPUT }}
        if: always()
      - name: upload-build-to-s3
        id: buildupload
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          echo "::set-output name=name::$(basename "${{ steps.slim.outputs.OUTPUT }}")"
          basename "${{ steps.slim.outputs.OUTPUT }}"
          aws s3 cp "${{ steps.slim.outputs.OUTPUT }}" s3://ta-staging-artifacts/ta-apps/
      - name: artifact-splunk-parts
        uses: actions/upload-artifact@v3
        with:
          name: package-deployment
          path: build/package/deployment**
        if: always()

  security-virustotal:
    continue-on-error: true
    name: security-virustotal
    needs: build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/download-artifact@v3
        with:
          name: package-splunkbase
          path: build/package/

      - name: VirusTotal Scan
        uses: crazy-max/ghaction-virustotal@v3
        with:
          vt_api_key: ${{ secrets.VT_API_KEY }}
          files: |
            build/package/*

  test-inventory:
    runs-on: ubuntu-latest
    # Map a step output to a job output
    outputs:
      unit: ${{ steps.testset.outputs.unit }}
      knowledge: ${{ steps.testset.outputs.knowledge }}
      ui: ${{ steps.testset.outputs.ui }}
      modinput_functional: ${{ steps.testset.outputs.modinput_functional }}
      requirement_test: ${{ steps.testset.outputs.requirement_test }}
      scripted_inputs: ${{ steps.testset.outputs.scripted_inputs }}
      escu: ${{ steps.testset.outputs.escu }}
    steps:
      - uses: actions/checkout@v3
      - id: testset
        name: testsets
        run: |
          find tests -type d -maxdepth 1 -mindepth 1 | sed 's|^tests/||g' |  while read -r TESTSET; do echo "::set-output name=$TESTSET::true"; echo "$TESTSET::true"; done

  run-unit-tests:
    name: test-unit-python3-${{ matrix.python-version }}
    if: ${{ needs.test-inventory.outputs.unit == 'true' }}
    runs-on: ubuntu-latest
    needs:
      - build
      - test-inventory
    strategy:
      fail-fast: false
      matrix:
        python-version:
          - "3.7"
    steps:
      - uses: actions/checkout@v3
      - name: Setup python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - uses: actions/download-artifact@v3
        with:
          name: package-raw
          path: output
      - name: Setup addon
        run: |
          if [ -f "poetry.lock" ]
           then
             mkdir -p package/lib || true
             pip install poetry==1.2.2 poetry-plugin-export==1.2.0
             poetry export --without-hashes -o package/lib/requirements.txt
             poetry export --without-hashes --dev -o requirements_dev.txt
           fi
          if [ ! -f requirements_dev.txt ]; then echo no requirements;exit 0 ;fi
          pip install -r requirements_dev.txt
      - name: Create directories
        run: |
          mkdir -p /opt/splunk/var/log/splunk
          chmod -R 777 /opt/splunk/var/log/splunk
      - name: Copy pytest ini
        run: cp tests/unit/pytest-ci.ini pytest.ini
      - name: Run Pytest with coverage
        run: pytest --cov=./ --cov-report=xml --junitxml=test-results/junit.xml tests/unit
      - name: Run Check if codecov enabled
        id: checkcodecov
        run: if [ -n "$CODECOV_TOKEN" ]; then echo "::set-output name=ENABLED::true"; fi
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
      - name: Upload coverage to Codecov
        if: ${{ steps.checkcodecov.outputs.ENABLED == 'true' }}
        uses: codecov/codecov-action@v3.1.0
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./coverage.xml
          directory: ./coverage/reports/
          env_vars: OS,PYTHON
          fail_ci_if_error: true
          path_to_write_report: ./coverage/codecov_report.txt
          verbose: true
      - uses: actions/upload-artifact@v3 # upload test results
        if: success() || failure() # run this step even if previous step failed
        with:
          name: test-results-unit-python_${{ matrix.python-version }}
          path: test-results/*

  run-requirements-unit-tests:
    if: ${{ needs.test-inventory.outputs.requirement_test == 'true' }}
    runs-on: ubuntu-latest
    needs:
      - build
      - test-inventory
    steps:
      - uses: actions/checkout@v3
      - name: Install Python 3
        uses: actions/setup-python@v4
        with:
          python-version: 3.7
      - name: run-tests
        uses: splunk/addonfactory-workflow-requirement-files-unit-tests@v1.4
        with:
          input-files: tests/requirement_test/logs
      - name: Archive production artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: |
            test_*.txt

  appinspect:
    name: quality-appinspect-${{ matrix.tags }}
    needs: build
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        tags:
          - "cloud"
          - "appapproval"
          - "deprecated_feature"
          - "developer_guidance"
          - "future"
          - "self-service"
          - "splunk_appinspect"
          - "manual"
    steps:
      - uses: actions/checkout@v3
      - uses: actions/download-artifact@v3
        with:
          name: package-splunkbase
          path: build/package/
      - name: Scan
        uses: splunk/appinspect-cli-action@v1.4
        with:
          app_path: build/package/
          included_tags: ${{ matrix.tags }}
      - name: upload-appinspect-report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: appinspect_${{ matrix.tags }}_checks.json
          path: appinspect_result.json
      - name: upload-manual-check-markdown
        if: matrix.tags == 'manual'
        uses: actions/upload-artifact@v3
        with:
          name: check_markdown.txt
          path: manual_check_markdown.txt

  artifact-registry:
    runs-on: ubuntu-latest
    needs:
      - security-virustotal
      - meta
    outputs:
      artifact: ${{ steps.artifactid.outputs.result }}
    steps:
      - uses: actions/checkout@v3
      - uses: actions/download-artifact@v3
        with:
          name: package-splunkbase
          path: build/package/splunkbase

      - id: getappid
        run: |
          appid=$(jq -r '.info.id.name' package/app.manifest)
          echo appid="$appid"
          echo "::set-output name=result::$appid"
      - run: |
          curl -LO https://github.com/oras-project/oras/releases/download/v0.12.0/oras_0.12.0_linux_amd64.tar.gz
          mkdir -p oras-install/
          tar -zxf oras_0.12.0_*.tar.gz -C oras-install/
          mv oras-install/oras /usr/local/bin/
          rm -rf oras_0.12.0_*.tar.gz oras-install/
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      - name: Login to GitHub Packages Docker Registry
        uses: docker/login-action@v2.0.0
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GH_TOKEN }}

      - name: Docker meta
        id: meta
        uses: docker/metadata-action@v4.0.1
        with:
          images: ghcr.io/${{ github.repository }}
          tags: |
            type=semver,pattern=v{{major}}.{{minor}},prefix=${{ steps.getappid.outputs.result }}-
            type=semver,pattern=v{{major}},prefix=${{ steps.getappid.outputs.result }}-
            type=semver,pattern=v{{version}},prefix=${{ steps.getappid.outputs.result }}-
            type=semver,pattern={{major}}.{{minor}},prefix=${{ steps.getappid.outputs.result }}-
            type=semver,pattern={{major}},prefix=${{ steps.getappid.outputs.result }}-
            type=semver,pattern={{version}},prefix=${{ steps.getappid.outputs.result }}-
            type=ref,event=branch,prefix=${{ steps.getappid.outputs.result }}-
            type=ref,event=pr,prefix=${{ steps.getappid.outputs.result }}-
            type=sha,prefix=${{ steps.getappid.outputs.result }}-
            type=sha,format=long,prefix=${{ steps.getappid.outputs.result }}-
      - name: Upload artifacts
        run: |
          tee /tmp/tags &>/dev/null <<EOF
          ${{ steps.meta.outputs.tags }}
          EOF
          pushd build/package/splunkbase/
          PACKAGE=$(ls ./*)
          echo "$PACKAGE"
          mv "$PACKAGE" "${{ steps.getappid.outputs.result }}".spl
          while IFS= read -r line
          do
            echo ">>$line<<"
            oras push \
                --manifest-config /dev/null:application/vnd.splunk.ent.package.v1.tar+gzip \
                "$line" \
                "${{ steps.getappid.outputs.result }}".spl
            echo "  complete"
          done < /tmp/tags
          popd
      - name: Output artifact locator
        id: artifactid
        run: |
          echo "::set-output name=result:: ${{ needs.meta.outputs.sc4s }}"

  setup:
    needs:
      - build
      - test-inventory
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/splunk/workflow-engine-base:2.0.3
    outputs:
      argo-server: ${{ steps.test-setup.outputs.argo-server }}
      argo-http1: ${{ steps.test-setup.outputs.argo-http1 }}
      argo-secure: ${{ steps.test-setup.outputs.argo-secure }}
      spl-host-suffix: ${{ steps.test-setup.outputs.spl-host-suffix }}
      argo-href: ""
      argo-base-href: ${{ steps.test-setup.outputs.argo-base-href }}
      argo-workflow-tmpl-name: ${{ steps.test-setup.outputs.argo-workflow-tmpl-name }}
      k8s-manifests-branch: ${{ steps.test-setup.outputs.k8s-manifests-branch }}
      argo-namespace: ${{ steps.test-setup.outputs.argo-namespace }}
      addon-name: ${{ steps.test-setup.outputs.addon-name }}
      job-name: ${{ steps.test-setup.outputs.job-name }}
      labels: ${{ steps.test-setup.outputs.labels }}
      addon-upload-path: ${{ steps.test-setup.outputs.addon-upload-path }}
      directory-path: ${{ steps.test-setup.outputs.directory-path }}
      s3-bucket: ${{ steps.test-setup.outputs.s3-bucket }}
      delay-destroy-ko: ${{ steps.delay-destroy-setup.outputs.delay-destroy-ko }}
      delay-destroy-ui: ${{ steps.delay-destroy-setup.outputs.delay-destroy-ui }}
      delay-destroy-modinput: ${{ steps.delay-destroy-setup.outputs.delay-destroy-modinput }}
      delay-destroy-escu: ${{ steps.delay-destroy-setup.outputs.delay-destroy-escu }}
      delay-destroy-scripted_inputs: ${{ steps.delay-destroy-setup.outputs.delay-destroy-scripted_inputs }}
      delay-destroy-requirement_test: ${{ steps.delay-destroy-setup.outputs.delay-destroy-requirement_test }}
      execute-ko: ${{ steps.delay-destroy-setup.outputs.execute-ko }}
      execute-ui: ${{ steps.delay-destroy-setup.outputs.execute-ui }}
      execute-escu: ${{ steps.delay-destroy-setup.outputs.execute-escu }}
      execute-modinput: ${{ steps.delay-destroy-setup.outputs.execute-modinput }}
      execute-scripted-inputs: ${{ steps.delay-destroy-setup.outputs.execute-scripted-inputs }}
      execute-requirement-test: ${{ steps.delay-destroy-setup.outputs.execute-requirement-test }}
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: setup for test
        id: test-setup
        shell: bash
        run: |
          echo "::set-output name=argo-server::argo.staging.wfe.splgdi.com:443"
          echo "::set-output name=argo-http1::true"
          echo "::set-output name=argo-secure::true"
          echo "::set-output name=spl-host-suffix::staging.wfe.splgdi.com"
          echo "::set-output name=argo-base-href::\'\'"
          echo "::set-output name=argo-namespace::workflows"
          echo "::set-output name=argo-workflow-tmpl-name::ta-workflow-jsonnet"
          echo "::set-output name=k8s-manifests-branch::jsonnet-enhance"

          ADDON_NAME=$(crudini --get package/default/app.conf id name | tr '[:lower:]' '[:upper:]')
          if [[ -n $(echo "${ADDON_NAME}" | awk -F 'SPLUNK_TA_' '{print $2}') ]];
          then
              ADDON_NAME=$(echo "${ADDON_NAME}" | awk -F 'SPLUNK_TA_' '{print $2}')
          elif [[ -n $(echo "${ADDON_NAME}" | awk -F '_FOR_SPLUNK' '{print $1}') ]];
          then
              ADDON_NAME=$(echo "${ADDON_NAME}" | awk -F '_FOR_SPLUNK' '{print $1}')
          fi
          echo "::set-output name=addon-name::\"$ADDON_NAME\""

          JOB_NAME=$(echo "$ADDON_NAME" | tail -c 16)-$(echo "${GITHUB_SHA}" | tail -c 8)-TEST-TYPE-${GITHUB_RUN_ID}
          JOB_NAME=${JOB_NAME//[_.]/-}
          echo "::set-output name=job-name::wf-$JOB_NAME"

          LABELS="addon-name=${ADDON_NAME}"
          echo "::set-output name=labels::$LABELS"

          ADDON_UPLOAD_PATH="s3://ta-staging-artifacts/ta-apps/${{ needs.build.outputs.buildname }}"
          echo "::set-output name=addon-upload-path::$ADDON_UPLOAD_PATH"
          echo "::set-output name=directory-path::/tmp"
          echo "::set-output name=s3-bucket::ta-staging-artifacts"

      - name: set delay destroy
        id: delay-destroy-setup
        shell: bash
        run: |
          set +e
          TESTSET="knowledge ui modinput_functional scripted_inputs escu requirement_test"
          for i in $TESTSET; do
              eval DELAY_DESTROY_$i="No"
              eval EXECUTE_$i="Yes"
          done
          if [[ '${{ github.event.label.name }}' == 'preserve_infra' ]]; then
            echo "${{ github.event.pull_request.body }}" >> body.txt
            tests=$(grep -i "^preserve:" body.txt | { grep -v grep || true; })
            if [[ $tests =~ "escu" ]]; then
              echo "preserve_infra for escu test-type is not supported yet"
            fi
            for i in $TESTSET; do
                if [[ $tests =~ "$i" ]]; then
                    eval DELAY_DESTROY_$i="Yes"
                else
                    eval EXECUTE_$i="No" 
                fi
            done
          fi
          # PRESERVE_INFRA for escu test-type is not supported yet.
          DELAY_DESTROY_escu="No"
          echo "::set-output name=delay-destroy-ko::$DELAY_DESTROY_knowledge"
          echo "::set-output name=delay-destroy-ui::$DELAY_DESTROY_ui"
          echo "::set-output name=delay-destroy-modinput::$DELAY_DESTROY_modinput_functional"
          echo "::set-output name=delay-destroy-scripted_inputs::$DELAY_DESTROY_scripted_inputs"
          echo "::set-output name=delay-destroy-escu::$DELAY_DESTROY_escu"
          echo "::set-output name=delay-destroy-requirement_test::$DELAY_DESTROY_requirement_test"
          echo "::set-output name=execute-ko::$EXECUTE_knowledge"
          echo "::set-output name=execute-ui::$EXECUTE_ui"
          echo "::set-output name=execute-modinput::$EXECUTE_modinput_functional"
          echo "::set-output name=execute-scripted-inputs::$EXECUTE_scripted_inputs"
          echo "::set-output name=execute-escu::$EXECUTE_escu"
          echo "::set-output name=execute-requirement-test::$EXECUTE_requirement_test"

  run-knowledge-tests:
    if: ${{ needs.test-inventory.outputs.knowledge == 'true' && needs.setup.outputs.execute-ko == 'Yes' }}
    needs:
      - build
      - test-inventory
      - setup
      - meta
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        splunk: ${{ fromJson(needs.meta.outputs.matrix_supportedSplunk) }}
        sc4s: ${{ fromJson(needs.meta.outputs.matrix_supportedSC4S) }}
    container:
      image: ghcr.io/splunk/workflow-engine-base:2.0.3
    env:
      ARGO_SERVER: ${{ needs.setup.outputs.argo-server }}
      ARGO_HTTP1: ${{ needs.setup.outputs.argo-http1 }}
      ARGO_SECURE: ${{ needs.setup.outputs.argo-secure }}
      ARGO_BASE_HREF: ${{ needs.setup.outputs.argo-href }}
      ARGO_NAMESPACE: ${{ needs.setup.outputs.argo-namespace }}
      SPLUNK_VERSION_BASE: ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }}
      TEST_TYPE: "knowledge"
      TEST_ARGS: ""
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
      - name: Read secrets from AWS Secrets Manager into environment variables
        id: get-argo-token
        run: |
          ARGO_TOKEN=$(aws secretsmanager get-secret-value --secret-id ta-staging-github-workflow-automation-token | jq -r '.SecretString')
          echo "::set-output name=argo-token::$ARGO_TOKEN"
      - name: create job name
        id: create-job-name
        shell: bash
        run: |
          RANDOM_STRING=$(head -3 /dev/urandom | tr -cd '[:lower:]' | cut -c -4)
          JOB_NAME=${{ needs.setup.outputs.job-name }}-${RANDOM_STRING}
          JOB_NAME=${JOB_NAME//TEST-TYPE/${{ env.TEST_TYPE }}}
          JOB_NAME=${JOB_NAME//[_.]/-}
          JOB_NAME=$(echo "$JOB_NAME" | tr '[:upper:]' '[:lower:]')
          echo "::set-output name=job-name::$JOB_NAME"
      - name: Splunk instance details
        id: splunk-instance-details
        if: ${{ needs.setup.outputs.delay-destroy-ko == 'Yes' }}
        shell: bash 
        run: |
          echo "Splunk will be accessible at https://${{ steps.create-job-name.outputs.job-name }}.${{ needs.setup.outputs.spl-host-suffix }}:8000 after test execution starts"
      - name: run-tests
        id: run-tests
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        uses: splunk/wfe-test-runner-action@v1.6
        with:
          splunk: ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }}
          test-type: ${{ env.TEST_TYPE }}
          test-args: ""
          job-name: ${{ steps.create-job-name.outputs.job-name }}
          labels: ${{ needs.setup.outputs.labels }}
          workflow-tmpl-name: ${{ needs.setup.outputs.argo-workflow-tmpl-name }}
          workflow-template-ns: ${{ needs.setup.outputs.argo-namespace }}
          delay-destroy: ${{ needs.setup.outputs.delay-destroy-ko }}
          addon-url: ${{ needs.setup.outputs.addon-upload-path }}
          addon-name: ${{ needs.setup.outputs.addon-name }}
          sc4s-version: ${{ matrix.sc4s.version }}
          sc4s-docker-registry: ${{ matrix.sc4s.docker_registry }}
          k8s-manifests-branch: ${{ needs.setup.outputs.k8s-manifests-branch }}
      - name: Read secrets from AWS Secrets Manager again into environment variables in case credential rotation
        id: update-argo-token
        run: |
          ARGO_TOKEN=$(aws secretsmanager get-secret-value --secret-id ta-staging-github-workflow-automation-token | jq -r '.SecretString')
          echo "::set-output name=argo-token::$ARGO_TOKEN"
      - name: Check if pod was deleted
        id: is-pod-deleted
        if: always()
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.update-argo-token.outputs.argo-token }}
        run: |
          set -o xtrace
          if argo watch ${{ steps.run-tests.outputs.workflow-name }} -n workflows | grep "pod deleted"; then
            echo "::set-output name=retry-workflow::true"
          fi
      - name: Retrying workflow
        id: retry-wf
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.update-argo-token.outputs.argo-token }}
        if: always()
        run: |
          set -o xtrace
          set +e
          if [[ "${{ steps.is-pod-deleted.outputs.retry-workflow }}" == "true" ]]
          then
            WORKFLOW_NAME=$(argo resubmit -v -o json -n workflows "${{ steps.run-tests.outputs.workflow-name }}" | jq -r .metadata.name)
            echo "::set-output name=workflow-name::$WORKFLOW_NAME"
            argo logs --follow "${WORKFLOW_NAME}" -n workflows || echo "... there was an error fetching logs, the workflow is still in progress. please wait for the workflow to complete ..."
          else
            echo "No retry required"
            argo wait "${{ steps.run-tests.outputs.workflow-name }}" -n workflows
            argo watch "${{ steps.run-tests.outputs.workflow-name }}" -n workflows | grep "test-addon"
          fi
      - name: check if workflow completed
        env:
          ARGO_TOKEN: ${{ steps.update-argo-token.outputs.argo-token }}
        shell: bash
        if: always()
        run: |
          set +e
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          echo "Status of workflow:" "$ARGO_STATUS"
          while [ "$ARGO_STATUS" == "Running"  ] || [ "$ARGO_STATUS" == "Pending" ]
          do
              echo "... argo Workflow ${WORKFLOW_NAME} is running, waiting for it to complete."
              argo wait "${WORKFLOW_NAME}" -n workflows || true
              ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          done
      - name: pull artifacts from s3 bucket
        if: always()
        run: |
          echo "pulling artifacts"
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/artifacts-${{ steps.create-job-name.outputs.job-name }}/${{ steps.create-job-name.outputs.job-name }}.tgz ${{ needs.setup.outputs.directory-path }}/
          tar -xf ${{ needs.setup.outputs.directory-path }}/${{ steps.create-job-name.outputs.job-name }}.tgz -C ${{ needs.setup.outputs.directory-path }}
      - name: pull logs from s3 bucket
        if: always()
        run: |
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          echo "pulling logs"
          mkdir -p ${{ needs.setup.outputs.directory-path }}/argo-logs
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/${WORKFLOW_NAME}/ ${{ needs.setup.outputs.directory-path }}/argo-logs/ --recursive
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} tests artifacts
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} tests logs
          path: |
            ${{ needs.setup.outputs.directory-path }}/argo-logs
      - name: Upload cim-compliance-report for ${{ matrix.splunk.version }}
        uses: actions/upload-artifact@v3
        if: ${{ matrix.splunk.islatest == true }}
        with:
          name: cim-compliance-report
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results/cim-compliance-report.md
      - name: Upload cim-field-report for ${{ matrix.splunk.version }}
        uses: actions/upload-artifact@v3
        if: ${{ matrix.splunk.islatest == true }}
        with:
          name: cim-field-report
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results/cim_field_report.json
      - name: Test Report
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} test report
          path: "${{ needs.setup.outputs.directory-path }}/test-results/*.xml"
          reporter: java-junit

  run-requirement-tests:
    if: ${{ needs.test-inventory.outputs.requirement_test == 'true' && needs.setup.outputs.execute-requirement-test == 'Yes' }}
    needs:
      - build
      - test-inventory
      - setup
      - meta
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        splunk: ${{ fromJson(needs.meta.outputs.matrix_latestSplunk) }}
        sc4s: ${{ fromJson(needs.meta.outputs.matrix_supportedSC4S) }}
    container:
      image: ghcr.io/splunk/workflow-engine-base:2.0.3
    env:
      ARGO_SERVER: ${{ needs.setup.outputs.argo-server }}
      ARGO_HTTP1: ${{ needs.setup.outputs.argo-http1 }}
      ARGO_SECURE: ${{ needs.setup.outputs.argo-secure }}
      ARGO_BASE_HREF: ${{ needs.setup.outputs.argo-href }}
      ARGO_NAMESPACE: ${{ needs.setup.outputs.argo-namespace }}
      TEST_TYPE: "requirement_test"
      TEST_ARGS: ""
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
      - name: Read secrets from AWS Secrets Manager into environment variables
        id: get-argo-token
        run: |
          ARGO_TOKEN=$(aws secretsmanager get-secret-value --secret-id ta-staging-github-workflow-automation-token | jq -r '.SecretString')
          echo "::set-output name=argo-token::$ARGO_TOKEN"
      - name: create job name
        id: create-job-name
        shell: bash
        run: |
          RANDOM_STRING=$(head -3 /dev/urandom | tr -cd '[:lower:]' | cut -c -4)
          JOB_NAME=${{ needs.setup.outputs.job-name }}-${RANDOM_STRING}
          JOB_NAME=${JOB_NAME//TEST-TYPE/${{ env.TEST_TYPE }}}
          JOB_NAME=${JOB_NAME//[_.]/-}
          JOB_NAME=$(echo "$JOB_NAME" | tr '[:upper:]' '[:lower:]')
          echo "::set-output name=job-name::$JOB_NAME"
      - name: Splunk instance details
        id: splunk-instance-details
        if: ${{ needs.setup.outputs.delay-destroy-requirement_test == 'Yes' }}
        shell: bash 
        run: |
          echo "Splunk will be accessible at https://${{ steps.create-job-name.outputs.job-name }}.${{ needs.setup.outputs.spl-host-suffix }}:8000 after test execution starts"
      - name: run-tests
        id: run-tests
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        uses: splunk/wfe-test-runner-action@v1.6
        with:
          splunk: ${{ matrix.splunk.version }}
          test-type: ${{ env.TEST_TYPE }}
          test-args: ""
          job-name: ${{ steps.create-job-name.outputs.job-name }}
          labels: ${{ needs.setup.outputs.labels }}
          workflow-tmpl-name: ${{ needs.setup.outputs.argo-workflow-tmpl-name }}
          workflow-template-ns: ${{ needs.setup.outputs.argo-namespace }}
          delay-destroy: ${{ needs.setup.outputs.delay-destroy-requirement_test }}
          addon-url: ${{ needs.setup.outputs.addon-upload-path }}
          addon-name: ${{ needs.setup.outputs.addon-name }}
          sc4s-version: ${{ matrix.sc4s.version }}
          sc4s-docker-registry: ${{ matrix.sc4s.docker_registry }}
          k8s-manifests-branch: ${{ needs.setup.outputs.k8s-manifests-branch }}
      - name: Check if pod was deleted
        id: is-pod-deleted
        if: always()
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        run: |
          set -o xtrace
          if argo watch ${{ steps.run-tests.outputs.workflow-name }} -n workflows | grep "pod deleted"; then
            echo "::set-output name=retry-workflow::true"
          fi
      - name: Retrying workflow
        id: retry-wf
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        if: always()
        run: |
          set -o xtrace
          set +e
          if [[ "${{ steps.is-pod-deleted.outputs.retry-workflow }}" == "true" ]]
          then
            WORKFLOW_NAME=$(argo resubmit -v -o json -n workflows "${{ steps.run-tests.outputs.workflow-name }}" | jq -r .metadata.name)
            echo "::set-output name=workflow-name::$$WORKFLOW_NAME"
            argo logs --follow "${WORKFLOW_NAME}" -n workflows || echo "... there was an error fetching logs, the workflow is still in progress. please wait for the workflow to complete ..."
          else
            echo "No retry required"
            argo wait "${{ steps.run-tests.outputs.workflow-name }}" -n workflows
            argo watch "${{ steps.run-tests.outputs.workflow-name }}" -n workflows | grep "test-addon"
          fi
      - name: check if workflow completed
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        shell: bash
        if: always()
        run: |
          set +e
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          echo "Status of workflow:" "$ARGO_STATUS"
          while [ "$ARGO_STATUS" == "Running"  ] || [ "$ARGO_STATUS" == "Pending" ]
          do
              echo "... argo Workflow ${WORKFLOW_NAME} is running, waiting for it to complete."
              argo wait "${WORKFLOW_NAME}" -n workflows || true
              ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          done
      - name: pull artifacts from s3 bucket
        if: always()
        run: |
          echo "pulling artifacts"
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/artifacts-${{ steps.create-job-name.outputs.job-name }}/${{ steps.create-job-name.outputs.job-name }}.tgz ${{ needs.setup.outputs.directory-path }}/
          tar -xf ${{ needs.setup.outputs.directory-path }}/${{ steps.create-job-name.outputs.job-name }}.tgz -C ${{ needs.setup.outputs.directory-path }}
      - name: pull logs from s3 bucket
        if: always()
        run: |
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          echo "pulling logs"
          mkdir -p ${{ needs.setup.outputs.directory-path }}/argo-logs
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/${WORKFLOW_NAME}/ ${{ needs.setup.outputs.directory-path }}/argo-logs/ --recursive
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ matrix.splunk.version }} ${{ env.TEST_TYPE }} tests artifacts
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ matrix.splunk.version }} ${{ env.TEST_TYPE }} tests logs
          path: |
            ${{ needs.setup.outputs.directory-path }}/argo-logs
      - name: Test Report
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: splunk ${{ matrix.splunk.version }} ${{ env.TEST_TYPE }} test report
          path: "${{ needs.setup.outputs.directory-path }}/test-results/*.xml"
          reporter: java-junit

  run-ui-tests:
    if: ${{ needs.test-inventory.outputs.ui == 'true' && needs.setup.outputs.execute-ui == 'Yes' }}
    needs:
      - build
      - test-inventory
      - setup
      - meta
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        splunk: ${{ fromJson(needs.meta.outputs.matrix_supportedSplunk) }}
        browser: [ "chrome","firefox" ]
        vendor-version: ${{ fromJson(needs.meta.outputs.matrix_supportedUIVendors) }}
    container:
      image: ghcr.io/splunk/workflow-engine-base:2.0.3
    env:
      ARGO_SERVER: ${{ needs.setup.outputs.argo-server }}
      ARGO_HTTP1: ${{ needs.setup.outputs.argo-http1 }}
      ARGO_SECURE: ${{ needs.setup.outputs.argo-secure }}
      ARGO_BASE_HREF: ${{ needs.setup.outputs.argo-href }}
      ARGO_NAMESPACE: ${{ needs.setup.outputs.argo-namespace }}
      SPLUNK_VERSION_BASE: ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }}
      TEST_TYPE: "ui"
      TEST_ARGS: "--browser ${{ matrix.browser }}"
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
      - name: Read secrets from AWS Secrets Manager into environment variables
        id: get-argo-token
        run: |
          ARGO_TOKEN=$(aws secretsmanager get-secret-value --secret-id ta-staging-github-workflow-automation-token | jq -r '.SecretString')
          echo "::set-output name=argo-token::$ARGO_TOKEN"
      - name: create job name
        id: create-job-name
        shell: bash
        run: |
          RANDOM_STRING=$(head -3 /dev/urandom | tr -cd '[:lower:]' | cut -c -4)
          JOB_NAME=${{ needs.setup.outputs.job-name }}-${RANDOM_STRING}
          JOB_NAME=${JOB_NAME//TEST-TYPE/${{ env.TEST_TYPE }}-${{ matrix.browser }}}
          JOB_NAME=${JOB_NAME//[_.:]/-}
          JOB_NAME=$(echo "$JOB_NAME" | tr '[:upper:]' '[:lower:]')
          echo "::set-output name=job-name::$JOB_NAME"
      - name: Splunk instance details
        id: splunk-instance-details
        if: ${{ needs.setup.outputs.delay-destroy-ui == 'Yes' }}
        shell: bash 
        run: |
          echo "Splunk will be accessible at https://${{ steps.create-job-name.outputs.job-name }}.${{ needs.setup.outputs.spl-host-suffix }}:8000 after test execution starts"
      - name: run-tests
        id: run-tests
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        uses: splunk/wfe-test-runner-action@v1.6
        with:
          splunk: ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }}
          test-type: ${{ env.TEST_TYPE }}
          test-args: ${{ env.TEST_ARGS }}
          job-name: ${{ steps.create-job-name.outputs.job-name }}
          labels: ${{ needs.setup.outputs.labels }}
          workflow-tmpl-name: ${{ needs.setup.outputs.argo-workflow-tmpl-name }}
          workflow-template-ns: ${{ needs.setup.outputs.argo-namespace }}
          delay-destroy: ${{ needs.setup.outputs.delay-destroy-ui }}
          addon-url: ${{ needs.setup.outputs.addon-upload-path }}
          addon-name: ${{ needs.setup.outputs.addon-name }}
          vendor-version: ${{ matrix.vendor-version.image }}
          sc4s-version: "No"
          k8s-manifests-branch: ${{ needs.setup.outputs.k8s-manifests-branch }}
      - name: Read secrets from AWS Secrets Manager again into environment variables in case credential rotation
        id: update-argo-token
        run: |
          ARGO_TOKEN=$(aws secretsmanager get-secret-value --secret-id ta-staging-github-workflow-automation-token | jq -r '.SecretString')
          echo "::set-output name=argo-token::$ARGO_TOKEN"
      - name: Check if pod was deleted
        id: is-pod-deleted
        if: always()
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.update-argo-token.outputs.argo-token }}
        run: |
          set -o xtrace
          if argo watch ${{ steps.run-tests.outputs.workflow-name }} -n workflows | grep "pod deleted" ; then
            echo "::set-output name=retry-workflow::true"
          fi
      - name: Retrying workflow
        id: retry-wf
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.update-argo-token.outputs.argo-token }}
        if: always()
        run: |
          set -o xtrace
          set +e
          if [[ "${{ steps.is-pod-deleted.outputs.retry-workflow }}" == "true" ]]
          then
            WORKFLOW_NAME=$(argo resubmit -v -o json -n workflows "${{ steps.run-tests.outputs.workflow-name }}" | jq -r .metadata.name)
            echo "::set-output name=workflow-name::$WORKFLOW_NAME"
            argo logs --follow "${WORKFLOW_NAME}" -n workflows || echo "... there was an error fetching logs, the workflow is still in progress. please wait for the workflow to complete ..."
          else
            echo "No retry required"
            argo wait "${{ steps.run-tests.outputs.workflow-name }}" -n workflows
            argo watch "${{ steps.run-tests.outputs.workflow-name }}" -n workflows | grep "test-addon"
          fi
      - name: check if workflow completed
        env:
          ARGO_TOKEN: ${{ steps.update-argo-token.outputs.argo-token }}
        if: always()
        shell: bash
        run: |
          set +e
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          echo "Status of workflow:" "$ARGO_STATUS"
          while [ "$ARGO_STATUS" == "Running"  ] || [ "$ARGO_STATUS" == "Pending" ]
          do
              echo "... argo Workflow ${WORKFLOW_NAME} is running, waiting for it to complete."
              argo wait "${WORKFLOW_NAME}" -n workflows || true
              ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          done
      - name: pull artifacts from s3 bucket
        if: always()
        run: |
          echo "pulling artifacts"
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/artifacts-${{ steps.create-job-name.outputs.job-name }}/${{ steps.create-job-name.outputs.job-name }}.tgz ${{ needs.setup.outputs.directory-path }}/
          tar -xf ${{ needs.setup.outputs.directory-path }}/${{ steps.create-job-name.outputs.job-name }}.tgz -C ${{ needs.setup.outputs.directory-path }}
      - name: pull logs from s3 bucket
        if: always()
        run: |
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          echo "pulling logs"
          mkdir -p ${{ needs.setup.outputs.directory-path }}/argo-logs
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/${WORKFLOW_NAME}/ ${{ needs.setup.outputs.directory-path }}/argo-logs/ --recursive
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.browser }} ${{ matrix.vendor-version.image }} tests artifacts
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.browser }} ${{ matrix.vendor-version.image }} tests logs
          path: |
            ${{ needs.setup.outputs.directory-path }}/argo-logs
      - name: Test Report
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.browser }} ${{ matrix.vendor-version.image }} test report
          path: "${{ needs.setup.outputs.directory-path }}/test-results/*.xml"
          reporter: java-junit

  run-modinput-tests:
    if: ${{ needs.test-inventory.outputs.modinput_functional == 'true' && needs.setup.outputs.execute-modinput == 'Yes' }}
    needs:
      - build
      - test-inventory
      - setup
      - meta
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        splunk: ${{ fromJson(needs.meta.outputs.matrix_supportedSplunk) }}
        modinput-type: [ "modinput_functional" ]
        vendor-version: ${{ fromJson(needs.meta.outputs.matrix_supportedModinputFunctionalVendors) }}
        marker: ${{ fromJson(inputs.marker) }}
    container:
      image: ghcr.io/splunk/workflow-engine-base:2.0.3
    env:
      ARGO_SERVER: ${{ needs.setup.outputs.argo-server }}
      ARGO_HTTP1: ${{ needs.setup.outputs.argo-http1 }}
      ARGO_SECURE: ${{ needs.setup.outputs.argo-secure }}
      ARGO_BASE_HREF: ${{ needs.setup.outputs.argo-href }}
      ARGO_NAMESPACE: ${{ needs.setup.outputs.argo-namespace }}
      SPLUNK_VERSION_BASE: ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }}
      TEST_TYPE: "modinput_functional"
      TEST_ARGS: ""
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
      - name: Read secrets from AWS Secrets Manager into environment variables
        id: get-argo-token
        run: |
          ARGO_TOKEN=$(aws secretsmanager get-secret-value --secret-id ta-staging-github-workflow-automation-token | jq -r '.SecretString')
          echo "::set-output name=argo-token::$ARGO_TOKEN"
      - name: create job name
        id: create-job-name
        shell: bash
        run: |
          RANDOM_STRING=$(head -3 /dev/urandom | tr -cd '[:lower:]' | cut -c -4)
          JOB_NAME=${{ needs.setup.outputs.job-name }}-${RANDOM_STRING}
          JOB_NAME=${JOB_NAME//TEST-TYPE/${{ env.TEST_TYPE }}}
          JOB_NAME=${JOB_NAME//[_.]/-}
          JOB_NAME=$(echo "$JOB_NAME" | tr '[:upper:]' '[:lower:]')
          echo "::set-output name=job-name::$JOB_NAME"
      - name: Splunk instance details
        id: splunk-instance-details
        if: ${{ needs.setup.outputs.delay-destroy-modinput == 'Yes' }}
        shell: bash 
        run: |
          echo "Splunk will be accessible at https://${{ steps.create-job-name.outputs.job-name }}.${{ needs.setup.outputs.spl-host-suffix }}:8000 after test execution starts"
      - name: create test argument
        id: create-test-arg
        shell: bash
        run: |
          export comparing_variable="[]"
          if [ "${{ inputs.marker }}" == "$comparing_variable" ]
          then
            TEST_ARG_M=""
          else
            TEST_ARG_M="-m"
          fi
          echo "::set-output name=test-arg::$TEST_ARG_M"
      - name: run-tests
        id: run-tests
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        uses: splunk/wfe-test-runner-action@v1.6
        with:
          splunk: ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }}
          test-type: ${{ env.TEST_TYPE }}
          test-args: ${{ env.TEST_ARGS }} ${{ steps.create-test-arg.outputs.test-arg }} ${{ matrix.marker }}
          job-name: ${{ steps.create-job-name.outputs.job-name }}
          labels: ${{ needs.setup.outputs.labels }}
          workflow-tmpl-name: ${{ needs.setup.outputs.argo-workflow-tmpl-name }}
          workflow-template-ns: ${{ needs.setup.outputs.argo-namespace }}
          delay-destroy: ${{ needs.setup.outputs.delay-destroy-modinput }}
          addon-url: ${{ needs.setup.outputs.addon-upload-path }}
          addon-name: ${{ needs.setup.outputs.addon-name }}
          vendor-version: ${{ matrix.vendor-version.image }}
          sc4s-version: "No"
          k8s-manifests-branch: ${{ needs.setup.outputs.k8s-manifests-branch }}
      - name: Read secrets from AWS Secrets Manager again into environment variables in case credential rotation
        id: update-argo-token
        run: |
          ARGO_TOKEN=$(aws secretsmanager get-secret-value --secret-id ta-staging-github-workflow-automation-token | jq -r '.SecretString')
          echo "::set-output name=argo-token::$ARGO_TOKEN"
      - name: Check if pod was deleted
        id: is-pod-deleted
        if: always()
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.update-argo-token.outputs.argo-token }}
        run: |
          set -o xtrace
          if argo watch ${{ steps.run-tests.outputs.workflow-name }} -n workflows | grep "pod deleted"; then
            echo "::set-output name=retry-workflow::true"
          fi
      - name: Retrying workflow
        id: retry-wf
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.update-argo-token.outputs.argo-token }}
        if: always()
        run: |
          set -o xtrace
          set +e
          if [[ "${{ steps.is-pod-deleted.outputs.retry-workflow }}" == "true" ]]
          then
            WORKFLOW_NAME=$(argo resubmit -v -o json -n workflows "${{ steps.run-tests.outputs.workflow-name }}" | jq -r .metadata.name)
            echo "::set-output name=workflow-name::$WORKFLOW_NAME"
            argo logs --follow "${WORKFLOW_NAME}" -n workflows || echo "... there was an error fetching logs, the workflow is still in progress. please wait for the workflow to complete ..."
          else
            echo "No retry required"
            argo wait "${{ steps.run-tests.outputs.workflow-name }}" -n workflows
            argo watch "${{ steps.run-tests.outputs.workflow-name }}" -n workflows | grep "test-addon"
          fi
      - name: check if workflow completed
        env:
          ARGO_TOKEN: ${{ steps.update-argo-token.outputs.argo-token }}
        if: always()
        shell: bash
        run: |
          set +e
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          echo "Status of workflow:" "$ARGO_STATUS"
          while [ "$ARGO_STATUS" == "Running"  ] || [ "$ARGO_STATUS" == "Pending" ]
          do
              echo "... argo Workflow ${WORKFLOW_NAME} is running, waiting for it to complete."
              argo wait "${WORKFLOW_NAME}" -n workflows || true
              ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          done
      - name: pull artifacts from s3 bucket
        if: always()
        run: |
          echo "pulling artifacts"
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/artifacts-${{ steps.create-job-name.outputs.job-name }}/${{ steps.create-job-name.outputs.job-name }}.tgz ${{ needs.setup.outputs.directory-path }}/
          tar -xf ${{ needs.setup.outputs.directory-path }}/${{ steps.create-job-name.outputs.job-name }}.tgz -C ${{ needs.setup.outputs.directory-path }}
      - name: pull logs from s3 bucket
        if: always()
        run: |
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          echo "pulling logs"
          mkdir -p ${{ needs.setup.outputs.directory-path }}/argo-logs
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/${WORKFLOW_NAME}/ ${{ needs.setup.outputs.directory-path }}/argo-logs/ --recursive
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} tests artifacts
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} tests logs
          path: |
            ${{ needs.setup.outputs.directory-path }}/argo-logs
      - name: Test Report
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} test report
          path: "${{ needs.setup.outputs.directory-path }}/test-results/*.xml"
          reporter: java-junit

  run-scripted-input-tests-full-matrix:
    if: ${{ needs.test-inventory.outputs.scripted_inputs == 'true' && ( github.base_ref == 'main' || github.ref_name == 'main' ) && needs.setup.outputs.execute-scripted-inputs == 'Yes' }}
    needs:
      - build
      - test-inventory
      - setup
      - meta
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        splunk: ${{ fromJson(needs.meta.outputs.matrix_supportedSplunk) }}
        os: [ "ubuntu:14.04", "ubuntu:16.04","ubuntu:18.04","ubuntu:22.04", "centos:7", "redhat:8.0", "redhat:8.2", "redhat:8.3", "redhat:8.4", "redhat:8.5" ]
    container:
      image: ghcr.io/splunk/workflow-engine-base:2.0.3
    env:
      ARGO_SERVER: ${{ needs.setup.outputs.argo-server }}
      ARGO_HTTP1: ${{ needs.setup.outputs.argo-http1 }}
      ARGO_SECURE: ${{ needs.setup.outputs.argo-secure }}
      ARGO_BASE_HREF: ${{ needs.setup.outputs.argo-href }}
      ARGO_NAMESPACE: ${{ needs.setup.outputs.argo-namespace }}
      SPLUNK_VERSION_BASE: ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }}
      TEST_TYPE: "scripted_inputs"
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
      - name: Read secrets from AWS Secrets Manager into environment variables
        id: get-argo-token
        run: |
          ARGO_TOKEN=$(aws secretsmanager get-secret-value --secret-id ta-staging-github-workflow-automation-token | jq -r '.SecretString')
          echo "::set-output name=argo-token::$ARGO_TOKEN"
      - name: create job name
        id: create-job-name
        shell: bash
        run: |
          RANDOM_STRING=$(head -3 /dev/urandom | tr -cd '[:lower:]' | cut -c -4)
          JOB_NAME=${{ needs.setup.outputs.job-name }}-${RANDOM_STRING}
          JOB_NAME=${JOB_NAME//TEST-TYPE/${{ env.TEST_TYPE }}}
          JOB_NAME=${JOB_NAME//[_.]/-}
          JOB_NAME=$(echo "$JOB_NAME" | tr '[:upper:]' '[:lower:]')
          echo "::set-output name=job-name::$JOB_NAME"
      - name: Splunk instance details
        id: splunk-instance-details
        if: ${{ needs.setup.outputs.delay-destroy-scripted_inputs == 'Yes' }}
        shell: bash 
        run: |
          echo "Splunk will be accessible at https://${{ steps.create-job-name.outputs.job-name }}.${{ needs.setup.outputs.spl-host-suffix }}:8000 after test execution starts"
      - name: get os name and version
        id: os-name-version
        shell: bash
        run: |
          OS_NAME_VERSION=${{ matrix.os }}
          # shellcheck disable=SC2206
          OS_NAME_VERSION=(${OS_NAME_VERSION//:/ })
          OS_NAME=${OS_NAME_VERSION[0]}
          OS_VERSION=${OS_NAME_VERSION[1]}
          echo "::set-output name=os-name::$OS_NAME"
          echo "::set-output name=os-version::$OS_VERSION"
      - name: run-tests
        id: run-tests
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        uses: splunk/wfe-test-runner-action@v1.6
        with:
          splunk: ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }}
          test-type: ${{ env.TEST_TYPE }}
          test-args: "--hostname=spl --os-name=${{ steps.os-name-version.outputs.os-name }} --os-version=${{ steps.os-name-version.outputs.os-version }} -m script_input"
          job-name: ${{ steps.create-job-name.outputs.job-name }}
          labels: ${{ needs.setup.outputs.labels }}
          workflow-tmpl-name: ${{ needs.setup.outputs.argo-workflow-tmpl-name }}
          workflow-template-ns: ${{ needs.setup.outputs.argo-namespace }}
          delay-destroy: ${{ needs.setup.outputs.delay-destroy-scripted_inputs }}
          addon-url: ${{ needs.setup.outputs.addon-upload-path }}
          addon-name: ${{ needs.setup.outputs.addon-name }}
          vendor-version: ${{ matrix.vendor-version.image }}
          sc4s-version: "No"
          os-name: ${{ steps.os-name-version.outputs.os-name }}
          os-version: ${{ steps.os-name-version.outputs.os-version }}
          k8s-manifests-branch: ${{ needs.setup.outputs.k8s-manifests-branch }}
      - name: Check if pod was deleted
        id: is-pod-deleted
        if: always()
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        run: |
          set -o xtrace
          if argo watch ${{ steps.run-tests.outputs.workflow-name }} -n workflows | grep "pod deleted"; then
            echo "::set-output name=retry-workflow::true"
          fi
      - name: Retrying workflow
        id: retry-wf
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        if: always()
        run: |
          set -o xtrace
          set +e
          if [[ "${{ steps.is-pod-deleted.outputs.retry-workflow }}" == "true" ]]
          then
            WORKFLOW_NAME=$(argo resubmit -v -o json -n workflows "${{ steps.run-tests.outputs.workflow-name }}" | jq -r .metadata.name)
            echo "::set-output name=workflow-name::$WORKFLOW_NAME"
            argo logs --follow "${WORKFLOW_NAME}" -n workflows || echo "... there was an error fetching logs, the workflow is still in progress. please wait for the workflow to complete ..."
          else
            echo "No retry required"
            argo wait "${{ steps.run-tests.outputs.workflow-name }}" -n workflows
            argo watch "${{ steps.run-tests.outputs.workflow-name }}" -n workflows | grep "test-addon"
          fi
      - name: check if workflow completed
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        if: always()
        shell: bash
        run: |
          set +e
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          echo "Status of workflow:" "$ARGO_STATUS"
          while [ "$ARGO_STATUS" == "Running"  ] || [ "$ARGO_STATUS" == "Pending" ]
          do
              echo "... argo Workflow ${WORKFLOW_NAME} is running, waiting for it to complete."
              argo wait "${WORKFLOW_NAME}" -n workflows || true
              ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          done
      - name: pull artifacts from s3 bucket
        if: always()
        run: |
          echo "pulling artifacts"
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/artifacts-${{ steps.create-job-name.outputs.job-name }}/${{ steps.create-job-name.outputs.job-name }}.tgz ${{ needs.setup.outputs.directory-path }}/
          tar -xf ${{ needs.setup.outputs.directory-path }}/${{ steps.create-job-name.outputs.job-name }}.tgz -C ${{ needs.setup.outputs.directory-path }}
      - name: pull logs from s3 bucket
        if: always()
        run: |
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          echo "pulling logs"
          mkdir -p ${{ needs.setup.outputs.directory-path }}/argo-logs
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/${WORKFLOW_NAME}/ ${{ needs.setup.outputs.directory-path }}/argo-logs/ --recursive
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} ${{ steps.os-name-version.outputs.os-name }} ${{ steps.os-name-version.outputs.os-version }} tests artifacts
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} ${{ steps.os-name-version.outputs.os-name }} ${{ steps.os-name-version.outputs.os-version }} tests logs
          path: |
            ${{ needs.setup.outputs.directory-path }}/argo-logs
      - name: Test Report
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} ${{ steps.os-name-version.outputs.os-name }} ${{ steps.os-name-version.outputs.os-version }}  test report
          path: "${{ needs.setup.outputs.directory-path }}/test-results/*.xml"
          reporter: java-junit

  run-scripted-input-tests-canary:
    if: ${{ needs.test-inventory.outputs.scripted_inputs == 'true' && ( github.base_ref == 'develop' || github.ref_name == 'develop' ) && needs.setup.outputs.execute-scripted-inputs == 'Yes' }}
    needs:
      - build
      - test-inventory
      - setup
      - meta
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        splunk: ${{ fromJson(needs.meta.outputs.matrix_supportedSplunk) }}
        os: [ "ubuntu:22.04", "centos:7","redhat:8.5" ]
    container:
      image: ghcr.io/splunk/workflow-engine-base:2.0.3
    env:
      ARGO_SERVER: ${{ needs.setup.outputs.argo-server }}
      ARGO_HTTP1: ${{ needs.setup.outputs.argo-http1 }}
      ARGO_SECURE: ${{ needs.setup.outputs.argo-secure }}
      ARGO_BASE_HREF: ${{ needs.setup.outputs.argo-href }}
      ARGO_NAMESPACE: ${{ needs.setup.outputs.argo-namespace }}
      SPLUNK_VERSION_BASE: ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }}
      TEST_TYPE: "scripted_inputs"
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
      - name: Read secrets from AWS Secrets Manager into environment variables
        id: get-argo-token
        run: |
          ARGO_TOKEN=$(aws secretsmanager get-secret-value --secret-id ta-staging-github-workflow-automation-token | jq -r '.SecretString')
          echo "::set-output name=argo-token::$ARGO_TOKEN"
      - name: create job name
        id: create-job-name
        shell: bash
        run: |
          RANDOM_STRING=$(head -3 /dev/urandom | tr -cd '[:lower:]' | cut -c -4)
          JOB_NAME=${{ needs.setup.outputs.job-name }}-${RANDOM_STRING}
          JOB_NAME=${JOB_NAME//TEST-TYPE/${{ env.TEST_TYPE }}}
          JOB_NAME=${JOB_NAME//[_.]/-}
          JOB_NAME=$(echo "$JOB_NAME" | tr '[:upper:]' '[:lower:]')
          echo "::set-output name=job-name::$JOB_NAME"
      - name: Splunk instance details 
        id: splunk-instance-details
        if: ${{ needs.setup.outputs.delay-destroy-scripted_inputs == 'Yes' }}
        shell: bash 
        run: |
          echo "Splunk will be accessible at https://${{ steps.create-job-name.outputs.job-name }}.${{ needs.setup.outputs.spl-host-suffix }}:8000 after test execution starts"
      - name: get os name and version
        id: os-name-version
        shell: bash
        run: |
          OS_NAME_VERSION=${{ matrix.os }}
          OS_NAME_VERSION=("${OS_NAME_VERSION//:/ }")
          OS_NAME=${OS_NAME_VERSION[0]}
          OS_VERSION=${OS_NAME_VERSION[1]}
          echo "::set-output name=os-name::$OS_NAME"
          echo "::set-output name=os-version::$OS_VERSION"
      - name: run-tests
        id: run-tests
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        uses: splunk/wfe-test-runner-action@v1.6
        with:
          splunk: ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }}
          test-type: ${{ env.TEST_TYPE }}
          test-args: "--hostname=spl --os-name=${{ steps.os-name-version.outputs.os-name }} --os-version=${{ steps.os-name-version.outputs.os-version }} -m script_input"
          job-name: ${{ steps.create-job-name.outputs.job-name }}
          labels: ${{ needs.setup.outputs.labels }}
          workflow-tmpl-name: ${{ needs.setup.outputs.argo-workflow-tmpl-name }}
          workflow-template-ns: ${{ needs.setup.outputs.argo-namespace }}
          delay-destroy: ${{ needs.setup.outputs.delay-destroy-scripted_inputs }}
          addon-url: ${{ needs.setup.outputs.addon-upload-path }}
          addon-name: ${{ needs.setup.outputs.addon-name }}
          vendor-version: ${{ matrix.vendor-version.image }}
          sc4s-version: "No"
          os-name: ${{ steps.os-name-version.outputs.os-name }}
          os-version: ${{ steps.os-name-version.outputs.os-version }}
          k8s-manifests-branch: ${{ needs.setup.outputs.k8s-manifests-branch }}
      - name: Check if pod was deleted
        id: is-pod-deleted
        if: always()
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        run: |
          set -o xtrace
          if argo watch ${{ steps.run-tests.outputs.workflow-name }} -n workflows | grep "pod deleted"; then
            echo "::set-output name=retry-workflow::true"
          fi
      - name: Retrying workflow
        id: retry-wf
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        if: always()
        run: |
          set -o xtrace
          set +e
          if [[ "${{ steps.is-pod-deleted.outputs.retry-workflow }}" == "true" ]]
          then
            WORKFLOW_NAME=$(argo resubmit -v -o json -n workflows "${{ steps.run-tests.outputs.workflow-name }}" | jq -r .metadata.name)
            echo "::set-output name=workflow-name::$WORKFLOW_NAME"
            argo logs --follow "${WORKFLOW_NAME}" -n workflows || echo "... there was an error fetching logs, the workflow is still in progress. please wait for the workflow to complete ..."
          else
            echo "No retry required"
            argo wait "${{ steps.run-tests.outputs.workflow-name }}" -n workflows
            argo watch "${{ steps.run-tests.outputs.workflow-name }}" -n workflows | grep "test-addon"
          fi
      - name: check if workflow completed
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        if: always()
        shell: bash
        run: |
          set +e
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          echo "Status of workflow:" "$ARGO_STATUS"
          while [ "$ARGO_STATUS" == "Running"  ] || [ "$ARGO_STATUS" == "Pending" ]
          do
              echo "... argo Workflow ${WORKFLOW_NAME} is running, waiting for it to complete."
              argo wait "${WORKFLOW_NAME}" -n workflows || true
              ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          done
      - name: pull artifacts from s3 bucket
        if: always()
        run: |
          echo "pulling artifacts"
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/artifacts-${{ steps.create-job-name.outputs.job-name }}/${{ steps.create-job-name.outputs.job-name }}.tgz ${{ needs.setup.outputs.directory-path }}/
          tar -xf ${{ needs.setup.outputs.directory-path }}/${{ steps.create-job-name.outputs.job-name }}.tgz -C ${{ needs.setup.outputs.directory-path }}
      - name: pull logs from s3 bucket
        if: always()
        run: |
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          echo "pulling logs"
          mkdir -p ${{ needs.setup.outputs.directory-path }}/argo-logs
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/${WORKFLOW_NAME}/ ${{ needs.setup.outputs.directory-path }}/argo-logs/ --recursive
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} ${{ steps.os-name-version.outputs.os-name }} ${{ steps.os-name-version.outputs.os-version }} tests artifacts
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: archive splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} ${{ steps.os-name-version.outputs.os-name }} ${{ steps.os-name-version.outputs.os-version }} tests logs
          path: |
            ${{ needs.setup.outputs.directory-path }}/argo-logs
      - name: Test Report
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} ${{ steps.os-name-version.outputs.os-name }} ${{ steps.os-name-version.outputs.os-version }}  test report
          path: "${{ needs.setup.outputs.directory-path }}/test-results/*.xml"
          reporter: java-junit

  run-escu-tests:
    if: ${{ needs.test-inventory.outputs.escu == 'true' && ( github.base_ref == 'main' || github.ref_name == 'main' || github.base_ref == 'develop' || github.ref_name == 'develop' ) && needs.setup.outputs.execute-escu == 'Yes' }}
    needs:
      - build
      - test-inventory
      - setup
      - meta
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        splunk: ${{ fromJson(needs.meta.outputs.matrix_latestSplunk) }}
    container:
      image: ghcr.io/splunk/workflow-engine-base:2.0.3
    env:
      ARGO_SERVER: ${{ needs.setup.outputs.argo-server }}
      ARGO_HTTP1: ${{ needs.setup.outputs.argo-http1 }}
      ARGO_SECURE: ${{ needs.setup.outputs.argo-secure }}
      ARGO_BASE_HREF: ${{ needs.setup.outputs.argo-href }}
      ARGO_NAMESPACE: ${{ needs.setup.outputs.argo-namespace }}
      SPLUNK_VERSION_BASE: ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }}
      TEST_TYPE: "escu"
    steps:
      - uses: actions/checkout@v3
        with:
          submodules: recursive
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
      - name: Read secrets from AWS Secrets Manager into environment variables
        id: get-argo-token
        run: |
          ARGO_TOKEN=$(aws secretsmanager get-secret-value --secret-id ta-staging-github-workflow-automation-token | jq -r '.SecretString')
          echo "::set-output name=argo-token::$ARGO_TOKEN"
      - name: create job name
        id: create-job-name
        shell: bash
        run: |
          RANDOM_STRING=$(head -3 /dev/urandom | tr -cd '[:lower:]' | cut -c -4)
          JOB_NAME=${{ needs.setup.outputs.job-name }}-${RANDOM_STRING}
          JOB_NAME=${JOB_NAME//TEST-TYPE/${{ env.TEST_TYPE }}}
          JOB_NAME=${JOB_NAME//[_.]/-}
          JOB_NAME=$(echo "$JOB_NAME" | tr '[:upper:]' '[:lower:]')
          echo "::set-output name=job-name::$JOB_NAME"
      - name: Splunk instance details
        id: splunk-instance-details
        if: ${{ needs.setup.outputs.delay-destroy-escu == 'Yes' }}
        shell: bash 
        run: |
          echo "Splunk will be accessible at https://${{ steps.create-job-name.outputs.job-name }}.${{ needs.setup.outputs.spl-host-suffix }}:8000 after test execution starts"
      - name: get escu detections
        id: get-escu-detections
        run: |
          RUN_TEST=false
          # shellcheck disable=SC2002
          DETECTIONS=$(cat tests/escu/.escu_detections | tr '\n' ',' | tr -d "[:space:]")
          if [ -z "$DETECTIONS" ]
          then
                echo "Detection list is empty."
          else
                RUN_TEST=true
          fi
          DETECTIONS="-tf $DETECTIONS"
          echo "::set-output name=escu-detections::$DETECTIONS"
          echo "::set-output name=escu-test-run::$RUN_TEST"
      - name: run-tests
        id: run-tests
        if: ${{ steps.get-escu-detections.outputs.escu-test-run == 'true' }}
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        uses: splunk/wfe-test-runner-action@v1.6
        with:
          splunk: ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }}
          test-type: ${{ env.TEST_TYPE }}
          test-args: ${{ steps.get-escu-detections.outputs.escu-detections }}
          job-name: ${{ steps.create-job-name.outputs.job-name }}
          labels: ${{ needs.setup.outputs.labels }}
          workflow-tmpl-name: ${{ needs.setup.outputs.argo-workflow-tmpl-name }}
          workflow-template-ns: ${{ needs.setup.outputs.argo-namespace }}
          delay-destroy: ${{ needs.setup.outputs.delay-destroy-escu }}
          addon-url: ${{ needs.setup.outputs.addon-upload-path }}
          addon-name: ${{ needs.setup.outputs.addon-name }}
          vendor-version: ${{ matrix.vendor-version.image }}
          sc4s-version: "No"
          k8s-manifests-branch: ${{ needs.setup.outputs.k8s-manifests-branch }}
      - name: Check if pod was deleted
        id: is-pod-deleted
        if: ${{ steps.get-escu-detections.outputs.escu-test-run == 'true' }}
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        run: |
          set -o xtrace
          if argo watch ${{ steps.run-tests.outputs.workflow-name }} -n workflows | grep "pod deleted"; then
            echo "::set-output name=retry-workflow::true"
          fi
      - name: Retrying workflow
        id: retry-wf
        shell: bash
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        if: ${{ steps.get-escu-detections.outputs.escu-test-run == 'true' }}
        run: |
          set -o xtrace
          set +e
          if [[ "${{ steps.is-pod-deleted.outputs.retry-workflow }}" == "true" ]]
          then
            WORKFLOW_NAME=$(argo resubmit -v -o json -n workflows "${{ steps.run-tests.outputs.workflow-name }}" | jq -r .metadata.name)
            echo "::set-output name=workflow-name::$WORKFLOW_NAME"
            argo logs --follow "${WORKFLOW_NAME}" -n workflows || echo "... there was an error fetching logs, the workflow is still in progress. please wait for the workflow to complete ..."
          else
            echo "No retry required"
            argo wait "${{ steps.run-tests.outputs.workflow-name }}" -n workflows
            argo watch "${{ steps.run-tests.outputs.workflow-name }}" -n workflows | grep "test-addon"
          fi
      - name: check if workflow completed
        env:
          ARGO_TOKEN: ${{ steps.get-argo-token.outputs.argo-token }}
        if: ${{ steps.get-escu-detections.outputs.escu-test-run == 'true' }}
        shell: bash
        run: |
          set +e
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          echo "Status of workflow:" "$ARGO_STATUS"
          while [ "$ARGO_STATUS" == "Running"  ] || [ "$ARGO_STATUS" == "Pending" ]
          do
              echo "... argo Workflow ${WORKFLOW_NAME} is running, waiting for it to complete."
              argo wait "${WORKFLOW_NAME}" -n workflows || true
              ARGO_STATUS=$(argo get "${WORKFLOW_NAME}" -n workflows -o json | jq -r '.status.phase')
          done
      - name: pull artifacts from s3 bucket
        if: ${{ steps.get-escu-detections.outputs.escu-test-run == 'true' }}
        run: |
          echo "pulling artifacts"
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/artifacts-${{ steps.create-job-name.outputs.job-name }}/${{ steps.create-job-name.outputs.job-name }}.tgz ${{ needs.setup.outputs.directory-path }}/
          tar -xf ${{ needs.setup.outputs.directory-path }}/${{ steps.create-job-name.outputs.job-name }}.tgz -C ${{ needs.setup.outputs.directory-path }}
      - name: pull logs from s3 bucket
        if: ${{ steps.get-escu-detections.outputs.escu-test-run == 'true' }}
        run: |
          # shellcheck disable=SC2157
          if [ -z "${{ steps.retry-wf.outputs.workflow-name }}" ]; then
            WORKFLOW_NAME=${{ steps.run-tests.outputs.workflow-name }}
          else
            WORKFLOW_NAME="${{ steps.retry-wf.outputs.workflow-name }}"
          fi
          echo "pulling logs"
          mkdir -p ${{ needs.setup.outputs.directory-path }}/argo-logs
          aws s3 cp s3://${{ needs.setup.outputs.s3-bucket }}/${WORKFLOW_NAME}/ ${{ needs.setup.outputs.directory-path }}/argo-logs/ --recursive
      - uses: actions/upload-artifact@v3
        if: ${{ steps.get-escu-detections.outputs.escu-test-run == 'true' }}
        with:
          name: archive splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} ${{ steps.os-name-version.outputs.os-name }} ${{ steps.os-name-version.outputs.os-version }} tests artifacts
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results
      - uses: actions/upload-artifact@v3
        if: ${{ steps.get-escu-detections.outputs.escu-test-run == 'true' }}
        with:
          name: archive splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} ${{ steps.os-name-version.outputs.os-name }} ${{ steps.os-name-version.outputs.os-version }} tests logs
          path: |
            ${{ needs.setup.outputs.directory-path }}/argo-logs
      - name: Upload results
        if: ${{ steps.get-escu-detections.outputs.escu-test-run == 'true' }}
        uses: actions/upload-artifact@v3
        with:
          name: escu-test-result
          path: |
            ${{ needs.setup.outputs.directory-path }}/test-results/escu-result.xml
      - name: Test Report
        uses: dorny/test-reporter@v1
        if: ${{ steps.get-escu-detections.outputs.escu-test-run == 'true' }}
        with:
          name: splunk ${{ matrix.splunk.version }}${{ secrets.OTHER_TA_REQUIRED_CONFIGS }} ${{ env.TEST_TYPE }} ${{ matrix.vendor-version.image }} test report
          path: "${{ needs.setup.outputs.directory-path }}/test-results/*.xml"
          reporter: java-junit

  validate-pr-title:
    name: Validate PR title
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    steps:
      - uses: amannn/action-semantic-pull-request@v4.5.0
        with:
          wip: true
          validateSingleCommit: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  pre-publish:
    if: always()
    needs:
      - meta
      - compliance-copyrights
      - lint
      - review_secrets
      - semgrep
      - build
      - security-virustotal
      - test-inventory
      - run-unit-tests
      - appinspect
      - setup
      - run-knowledge-tests
      - run-modinput-tests
      - run-ui-tests
      - validate-pr-title
    runs-on: ubuntu-latest
    env:
      NEEDS: ${{ toJson(needs) }}
    steps:
      - name: check if tests have passed or skipped
        id: check
        shell: bash
        run: |
          RUN_PUBLISH=$(echo "$NEEDS" | jq ".[] |  select(  ( .result != \"skipped\" ) and .result != \"success\" ) | length == 0")
          if [[ "$RUN_PUBLISH" != *'false'* ]]
          then
              echo "::set-output name=run-publish::true"
          else
              echo "::set-output name=run-publish::false"
          fi
      - name: exit without publish
        if: ${{ steps.check.outputs.run-publish == 'false' }}
        run: |
          echo " some test job failed. "
          exit 1

  publish:
    if: always() && needs.pre-publish.result == 'success' && github.event_name != 'pull_request' && github.event_name != 'schedule'
    needs:
      - pre-publish
      - run-escu-tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
        with:
          submodules: false
          persist-credentials: false
      - uses: actions/setup-node@v3
        with:
          node-version: '14'
      - name: Semantic Release
        id: semantic
        uses: cycjimmy/semantic-release-action@v2.7.0
        with:
          semantic_version: 17
          extra_plugins: |
            @semantic-release/exec
            @semantic-release/git
            @semantic-release/commit-analyzer
            @semantic-release/release-notes-generator
            @semantic-release/github
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN_ADMIN }}
      - name: Download package-deployment
        if: ${{ steps.semantic.outputs.new_release_published == 'true' }}
        uses: actions/download-artifact@v3
        id: download-package-deployment
        with:
          name: package-deployment
          path: download/artifacts/
      - name: Download package-splunkbase
        if: ${{ steps.semantic.outputs.new_release_published == 'true' }}
        uses: actions/download-artifact@v3
        id: download-package-splunkbase
        with:
          name: package-splunkbase
          path: download/artifacts/deployment
      - name: Download cim-compliance-report
        id: download-cim-compliance-report
        if: ${{ steps.semantic.outputs.new_release_published == 'true' }}
        continue-on-error: true
        uses: actions/download-artifact@v3
        with:
          name: cim-compliance-report
          path: download/artifacts/deployment
      - name: Download cim-field-report
        id: download-cim-field-report
        if: ${{ steps.semantic.outputs.new_release_published == 'true' }}
        continue-on-error: true
        uses: actions/download-artifact@v3
        with:
          name: cim-field-report
          path: download/artifacts/deployment
      - name: Download escu-test-results
        id: download-escu-xml
        if: ${{ steps.semantic.outputs.new_release_published == 'true' }}
        continue-on-error: true
        uses: actions/download-artifact@v3
        with:
          name: escu-test-result
          path: download/artifacts/deployment
      - name: List of assets
        if: ${{ steps.semantic.outputs.new_release_published == 'true' }}
        run: |
          ls -la ${{ steps.download-package-splunkbase.outputs.download-path }}
      - name: Upload assets to release
        if: ${{ steps.semantic.outputs.new_release_published == 'true' }}
        uses: svenstaro/upload-release-action@v2
        with:
          repo_token: ${{ secrets.GH_TOKEN_ADMIN }}
          file: ${{ steps.download-package-splunkbase.outputs.download-path }}/*
          overwrite: true
          file_glob: true
          tag: v${{ steps.semantic.outputs.new_release_version }}

  get_workflow_conclusion:
    if: always()
    needs: [ publish ]
    runs-on: ubuntu-latest
    steps:
      - name: Send logs to Skynet
        uses: splunk/collect-ta-logs@main
        with:
          git_token: ${{secrets.GH_TOKEN}}
          skynet-token: ${{ secrets.SKYNET_TOKEN }}
          skynet-url: "https://http-inputs-services-ingest.splunkcloud.com/services/collector/event"        

    